<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://namdarine.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://namdarine.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-20T01:57:48+00:00</updated><id>https://namdarine.github.io/feed.xml</id><title type="html">namdarine</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">ConvNeXt V2 Paper Review</title><link href="https://namdarine.github.io/blog/2025/ConvNeXt-V2-review/" rel="alternate" type="text/html" title="ConvNeXt V2 Paper Review"/><published>2025-03-19T18:00:00+00:00</published><updated>2025-03-19T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2025/ConvNeXt-V2-review</id><content type="html" xml:base="https://namdarine.github.io/blog/2025/ConvNeXt-V2-review/"><![CDATA[<blockquote> <p>This post summarizes “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders” (Sanghyun Woo et al., CVPR 2023) and shares an implementation based on the ideas presented in the paper. This post was written for learning purposes.</p> </blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> <p>ConvNeXt V2 is an improved model of ConvNeXt, a modern convolutional neural network (ConvNet), which significantly improves performance by co-designing self-supervised learning using <strong>Masked Autoencoder (MAE)</strong> and architectural design. Existing ConvNeXt was optimized for supervised learning, but when simply combined with MAE, its performance did not meet expectations.<br/> To solve this:</p> <ol> <li> <p>Fully Convolutional Masked Autoencoder (FCMAE): Transformer-optimized MAE transform to fit ConvNet. It handles masked inputs efficiently by introducing sparse convolution and replacing the decoder with a simple ConvNeXt block to make it a fully convolutional structure.</p> </li> <li> <p>Global Response Normalization (GRN): Add a new normalization layer that enhances feature competition between channels. This solved the feature collapse problem and increased expressiveness.</p> </li> </ol> <p>As a result, ConvNeXt V2 significantly improves the performance of pure ConvNet on various tasks such as ImageNet classification (up to 88.9% accuracy), COCO object detection and ADE20K segmentation. The model is scalable from 3.7M parameter (Atto) to 650M parameter (Huge), and achieves state-of-art performance with public data.</p> <h2 id="core-content-analysis">Core Content Analysis</h2> <h3 id="problem-definitions-and-mathematical-modeling">Problem definitions and Mathematical modeling</h3> <p>Existing ConvNeXt is well-optimized for supervised learning, but when combined with self-supervised learning, especially MAE, the performance is poor. Since the MAE is designed to be suitable for transformers, it creates compatibility issues with ConvNet’s dense sliding window scheme. Furthermore, the expressive power is degraded by “feature collapse” during mask-based pre-training on ConvNet.</p> <h3 id="defined-variables-and-expressions">Defined variables and expressions</h3> <ul> <li><strong>Masking</strong>: Generate a random mask <em>M</em> with 60% mask rate for input image <em>I</em>. <em>M</em> is defined in units of \(32 \times 32\) patches at the last stage and is tailored to the original resolution by upsampling.</li> <li><strong>Sparsal convolution</strong>: Processing only the pixels seen in input \(X \in \mathbb{R}^{H \times W \times C}\). Output \(Y = SparseConv(X, W)\), where \(W\) is learningable filters.</li> <li> <table> <tbody> <tr> <td><strong>GRN</strong>: Global aggregation $$ G(X) =</td> <td> </td> <td>X</td> <td> </td> <td>_{2} \(for input\) X \in \mathbb{R}^{H \times W \times C} \(, Normalization\) N(X) = \frac{X}{G(X)} \(, Correction\) Y = \gamma \cdot N(X) + \beta \((\) \gamma \(,\) \beta $$ are learnable parameters).</td> </tr> </tbody> </table> </li> <li><strong>Loss function</strong>: \(L = MSE(I_{masked}, \hat{I}_{masked})\) for masked patches, where \(\hat{I}\) is a reconstructed image.</li> </ul> <h3 id="assumtion">Assumtion</h3> <p>Suppose that if ConvNet is to be as suitable for mask-based self-supervised learning as Transformer, the architecture, and the learning framework must be co-designed. Assume that the feature collapse comes from a lack of competition between channels.</p> <h3 id="coonection-with-existing-research">Coonection with existing research</h3> <p>MAE has been successful on ViT (HE et al., 2022), but its effectiveness is limited on ConvNet (Jing et al., 2022). ConvNeXt V1 (Liu et al., 2022) has shown scalability in supervised learning, but self-supervised learning has been insufficient.</p> <h2 id="analyzing-the-suggested-methods">Analyzing the suggested methods</h2> <h3 id="core-algorithms-and-model-structures">Core Algorithms and Model Structures</h3> <ul> <li><strong>FCMAE</strong>: Fully convolutional MAE. The encoder applies sparse convolution to ConvNeXt, and the decoder consists of a single ConvNeXt block (512 dimensions). Masked inputs are viewed as sparse data and processed.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure2-480.webp 480w,/assets/img/paper1/figure2-800.webp 800w,/assets/img/paper1/figure2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/figure2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>GRN</strong>: Normalized layer added behind MLP layer. Global response (L2 norm) increases inter-channel contrast and enhances feature diversity.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/algorithm1-480.webp 480w,/assets/img/paper1/algorithm1-800.webp 800w,/assets/img/paper1/algorithm1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/algorithm1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>ConvNeXt V2</strong>: Integrate GRN into the ConvNeXt block and remove LayerScale. Model size expansion from Atto (3.7M) to Huge (650M).</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure5-480.webp 480w,/assets/img/paper1/figure5-800.webp 800w,/assets/img/paper1/figure5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/figure5.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="improvements-over-existing-research">Improvements over existing research</h3> <ul> <li>MAE (based on ViT) is optimized for transformers with an asymmetric encoder-decoder. FCMAE transforms to full convolution to fit ConvNet, improving efficiency with sparse convolution.</li> <li>SimMIM(based on Swin) and others are transformer-dependent. GRN resolves ConvNet-specific feature collapse and improves performance without additional parameters.</li> </ul> <h3 id="originality-of-the-technique">Originality of the technique</h3> <p>Sparse convolution is inspired by 3D point clouds (Choy et al., 2019), and GRN is an idea from lateral inhibition in neuroscience. Both are newly applied to ConvNet.</p> <h2 id="experimental-interpretations---key-concept-analysis">Experimental Interpretations - Key Concept Analysis</h2> <h3 id="experimental-methods-and-datasets">Experimental methods and datasets</h3> <h4 id="used-datasets">Used datasets</h4> <ul> <li><strong>ImageNet-1K</strong>: 1,000 classes, 1.3 million images. 800 epochs of pre-training, 100 epochs of fine-tuning</li> <li><strong>ImageNet-22K</strong>: 22,000 classes, used for intermediate fine tuning. $ 384 \times 384 $ resolution</li> <li><strong>COCO</strong>: Object detection and instance segmentation, evaluated with Mask R-CNN</li> <li><strong>ADE20K</strong>: Semantic segmentation, evaluated with UpperNet</li> <li><strong>Preprocessing</strong>: Minimum data augmentation (random crop only), patch-wise normalization</li> </ul> <h4 id="experimental-environment">Experimental environment</h4> <ul> <li><strong>Hardware</strong>: 256 core TPU-v3 pod (based on JAX)</li> <li><strong>Software</strong>: PyTorch/JAX, <a href="https://github.com/rwightman/pytorch-image-models">github</a></li> </ul> <h4 id="performance-comparison-indicators">Performance comparison indicators</h4> <ul> <li><strong>ImageNet</strong>: Top-1 accuracy (%)</li> <li><strong>COCO</strong>: \(mAP^{box}\) (detection), \(mAP^{mask}\) (segmentation)</li> <li><strong>ADE20K</strong>: $ mloU $ (Average intersection/union)</li> <li><strong>Comparative method</strong>: Compare to V1 (supervised), Compare V2 + FCMAE, and Transformer (Swin, ViT)</li> </ul> <h2 id="result">Result</h2> <h3 id="summary">Summary</h3> <ul> <li><strong>*ImageNet-1K</strong>: V2 + FCMAE is superior to supervised V1 (83.8% and 84.3%) with Base (84.6%), Large (85.6%), and Huge (86.3%). After IN-22K fine-tuning, Huge is 88.9% SOTA</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table5-480.webp 480w,/assets/img/paper1/table5-800.webp 800w,/assets/img/paper1/table5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/table5.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>COCO</strong>: V2 + FCMAE Huge is $ mAP^{box} $ 2.5 and $ mAP^{mask} $ 2.0 higher than Swin Huge</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table6-480.webp 480w,/assets/img/paper1/table6-800.webp 800w,/assets/img/paper1/table6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/table6.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>ADE20K</strong>: V2 + FCMAE Huge is mIoU 54.0, a significant improvement over supervised V1 (49.9)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table7-480.webp 480w,/assets/img/paper1/table7-800.webp 800w,/assets/img/paper1/table7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/table7.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="reasons-for-performance-improvement">Reasons for performance improvement</h3> <ul> <li><strong>FCMAE</strong>: Sparse convolutions prevent information leakage and simple decoders maintain efficiency</li> <li><strong>GRN</strong>: Improve expressiveness with feature collapse resolution</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure3-480.webp 480w,/assets/img/paper1/figure3-800.webp 800w,/assets/img/paper1/figure3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/figure3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure4-480.webp 480w,/assets/img/paper1/figure4-800.webp 800w,/assets/img/paper1/figure4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/figure4.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>V2 + FCMAE outperforms supervised V2 by 0.8 - 1.3%</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table3-480.webp 480w,/assets/img/paper1/table3-800.webp 800w,/assets/img/paper1/table3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/table3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>co-design</strong>: Synergy between architecture and learning framework achieves transformer-level performance.</li> </ul> <h2 id="generality">Generality</h2> <ul> <li>Not conditionally dependent: Consistent performance improvement at all sizes, from Atto to Huge</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure1-480.webp 480w,/assets/img/paper1/figure1-800.webp 800w,/assets/img/paper1/figure1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/figure1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Verify robust transfer learning performance on various tasks (ImageNet, COCO, ADE20K)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table6-480.webp 480w,/assets/img/paper1/table6-800.webp 800w,/assets/img/paper1/table6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/table6.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table7-480.webp 480w,/assets/img/paper1/table7-800.webp 800w,/assets/img/paper1/table7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/paper1/table7.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>However, the Huge model may be slightly data-scale dependent, as it has benefited more significantly from the IN-22K additional data</li> </ul>]]></content><author><name></name></author><category term="CV,"/><category term="paper_review"/><category term="WIS"/><summary type="html"><![CDATA[Paper review]]></summary></entry><entry><title type="html">RAG and LoRA</title><link href="https://namdarine.github.io/blog/2025/RAG&LoRA/" rel="alternate" type="text/html" title="RAG and LoRA"/><published>2025-03-01T18:00:00+00:00</published><updated>2025-03-01T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2025/RAG&amp;LoRA</id><content type="html" xml:base="https://namdarine.github.io/blog/2025/RAG&amp;LoRA/"><![CDATA[<h1 id="rag-retrieval-augmented-generation">RAG (Retrieval-Augmented Generation)</h1> <h2 id="what-is-rag">What is RAG?</h2> <p>RAG is a technique that helps to generate and up-to-date information by adding an information retrieval (Retrieval) function to the Large Language Model (LLM). While existing LLMs rely on pre-trained data to generate answers, RAG finds relevant information from an external database and augments the model’s response.<br/> This is cost-effective approach to improve LLM results to maintain relevance, accuracy, and utility in various situation.</p> <h2 id="key-components-of-rag">Key Components of RAG</h2> <h3 id="retriever">Retriever</h3> <ul> <li>Searches for relevant documents from a <strong>vector</strong> database, such as FAISS, Pinecone, Weaviate, Qdrant</li> <li>Uses BM25, Dense Passage Retrieval (DPR), or ColBERT for efficient information retrieval</li> </ul> <h3 id="generator">Generator</h3> <ul> <li>Uses the retrieved documents as context to generate text</li> <li>Typically based on transformer models like GPT, MPT or LLaMA</li> </ul> <h2 id="how-rag-works">How RAG Works</h2> <ol> <li> <p>User Query $ \rightangle $ Vector Representation</p> <ul> <li>The input query is transformed into a vector and compared against pre-stored documents in a vector database</li> </ul> </li> <li> <p>Document Retrieval</p> <ul> <li>The retriever fetches relevant documents based on similarity</li> </ul> </li> <li> <p>LLM Response Generation</p> <ul> <li>The LLM uses retrieved documents as additional context to generate a response</li> </ul> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rag-480.webp 480w,/assets/img/rag-800.webp 800w,/assets/img/rag-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rag.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="advantages">Advantages</h2> <ul> <li>Incorporates real-time data without retraining the model. This reflects the latest information even the model was not trained about.</li> <li>Reduces hallucinations by providing factual references. This improves reliability</li> <li>Allows customization for specific domains, such as legal, medical, finance</li> </ul> <p>It can adapt to changing requirements or departmental use by controlling and changing the information source of LLM. It can also allow developers to limit the search for sensitive information to different levels of authentication and allow LLM to generate appropriate responses. It can also troubleshoot and correct issues when LLM references sources of misinformation for specific questions. Organizations can more confidently implement Generative AI technologies for a wider range of applications.</p> <h1 id="lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</h1> <h2 id="what-is-lora">What is LoRA?</h2> <blockquote> <p>Low-Rank Adaptation is lightweight fine-tuning method that adapts LLMs <strong>without modifying their core weights</strong>. Instead of traing the entire models, LoRA adds small trainable matrices to selected layers.</p> </blockquote> <h2 id="key-concept-of-lora">Key Concept of LoRA</h2> <p>Instead of updating directly the weight matrix of the transformer model, LoRA introduces two smaller matrices that capture only the necessary modifications. That is, a method of learning only small additional matrices while maintaining the main weights of the original model and changing only the necessary parts.<br/> This method drastically reduces memory usage and computational costs while maintaining fine-tuning effectiveness.</p> <h2 id="how-lora-works">How LoRA Works</h2> <ol> <li>The pre-trained model weights (W) remain frozen and introduces low-rank matrices (A and B) to learn task-specific modifications.</li> <li>When there is input data, tune the model with the updated value of the original weights (W) + new low-rank matrices (A and B).</li> <li>After training, only saving the added matrices (A, B) can maintain the lightweight model while reducing GPU memory consumption.</li> </ol> <h2 id="advantages-of-lora">Advantages of LoRA</h2> <ul> <li> <h2 id="efficient-memory-usage-requires-significantly-less-gpu-vram-compared-with-full-fine-tuning"><strong>Efficient memory usage</strong>: requires significantly less GPU VRAM compared with Full Fine-Tuning</h2> </li> </ul> <h1 id="reference">Reference</h1> <ul> <li>image: What Is Rag? - Retrieval-Augmented Generation AI Explained - AWS, aws.amazon.com/what-is/retrieval-augmented-generation/. Accessed 12 Mar. 2025.</li> </ul>]]></content><author><name></name></author><category term="ML"/><category term="WIS"/><summary type="html"><![CDATA[Understand the concept of RAG and LoRA]]></summary></entry><entry><title type="html">How to deploy with AWS EC2 and Docker</title><link href="https://namdarine.github.io/blog/2025/deploy/" rel="alternate" type="text/html" title="How to deploy with AWS EC2 and Docker"/><published>2025-02-08T18:00:00+00:00</published><updated>2025-02-08T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2025/deploy</id><content type="html" xml:base="https://namdarine.github.io/blog/2025/deploy/"><![CDATA[<p>The following process assume created EC2 Instance with the Amazon linux x86 version.</p> <h1 id="ec2-instance-setting">EC2 Instance setting</h1> <h2 id="on-the-terminal">On the terminal</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-i</span> <span class="s2">"path/to/your-key.pem"</span> ec2-user@your-ec2-public-ip
</code></pre></div></div> <ul> <li>“path/to/your-key.pem” -&gt; EC2 key pair in use</li> <li>your-ec2-public-ip -&gt; EC2 public IP</li> </ul> <p>Then the terminal will be change to EC2 terminal.</p> <h2 id="update-basic-packages">Update basic packages</h2> <p>In the EC2 terminal</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum update <span class="nt">-y</span>
<span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> git nano unzip wget curl
</code></pre></div></div> <h1 id="clone-the-project-and-setting-variables">Clone the project and Setting variables</h1> <h2 id="clone-the-git-repository">Clone the Git repository</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /home/ec2-user
git clone https://github.com/your-repo-url.git
<span class="nb">cd </span>your-repo
</code></pre></div></div> <h2 id="setting-environment-file-env-if-necessary">Setting environment file (.env), if necessary</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano .env
</code></pre></div></div> <p>Then it will show nano editor. Store any necessary information should not be leaked, such as api-key</p> <h3 id="example">Example</h3> <pre><code class="language-nano">AWS_ACCESS_KEY_ID=your-aws-key
AWS_SECRET_ACCESS_KEY=your-aws-secret-key
</code></pre> <h3 id="important-note">Important Note</h3> <p>This .env file should be add to .gitignore to prevent it is uploaded to git.</p> <h1 id="setting-nginx-and-issuance-of-https-certificates">Setting Nginx and Issuance of HTTPS certificates</h1> <h2 id="download-nginx">Download Nginx</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>amazon-linux-extras <span class="nb">enable </span>nginx1
<span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> nginx
<span class="nb">sudo </span>systemctl start nginx
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>nginx
</code></pre></div></div> <ul> <li>Check Nginx is working properly</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl status nginx
</code></pre></div></div> <ul> <li>AWS security group should allow 80 (HTTP) and 443 (HTTPS) ports.</li> </ul> <h2 id="delete-preferences-file-and-add-new-settings">Delete Preferences File and Add New Settings</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo rm</span> <span class="nt">-f</span> /etc/nginx/nginx.conf
<span class="nb">sudo </span>nano /etc/nginx/nginx.conf
</code></pre></div></div> <p>Then it will pop up the nano editor and need to edit following.</p> <div class="language-nginx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">server</span> <span class="p">{</span>
    <span class="kn">listen</span> <span class="mi">80</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">your-domain.com</span> <span class="s">www.your-domain.com</span><span class="p">;</span>

    <span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
        <span class="kn">return</span> <span class="mi">301</span> <span class="s">https://</span><span class="nv">$host$request_uri</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">server</span> <span class="p">{</span>
    <span class="kn">listen</span> <span class="mi">443</span> <span class="s">ssl</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">your-domain.com</span> <span class="s">www.your-domain.com</span><span class="p">;</span>

    <span class="kn">ssl_certificate</span> <span class="n">/etc/letsencrypt/live/your-domain.com/fullchain.pem</span><span class="p">;</span>
    <span class="kn">ssl_certificate_key</span> <span class="n">/etc/letsencrypt/live/your-domain.com/privkey.pem</span><span class="p">;</span>

    <span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
        <span class="kn">proxy_pass</span> <span class="s">http://Flask-container-port</span><span class="p">;</span>  <span class="c1"># Flask 컨테이너 포트</span>
        <span class="kn">proxy_set_header</span> <span class="s">Host</span> <span class="nv">$host</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">X-Real-IP</span> <span class="nv">$remote_addr</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">X-Forwarded-For</span> <span class="nv">$proxy_add_x_forwarded_for</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>After edit control+X -&gt; y -&gt; enter (Mac user)</p> <h2 id="test-and-apply-nginx-settings">Test and apply Nginx settings</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nginx <span class="nt">-t</span>
</code></pre></div></div> <p>If it shows</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf <span class="nb">test </span>is successful
</code></pre></div></div> <p>then ready to go with the current setting.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl restart nginx
</code></pre></div></div> <p>Restart the Nginx.</p> <h2 id="issue-lets-encrypt-ssl-certificate">Issue Let’s Encrypt SSL Certificate</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> certbot python3-certbot-nginx
<span class="nb">sudo </span>certbot <span class="nt">--nginx</span> <span class="nt">-d</span> your-domain.com <span class="nt">-d</span> www.your-domain.com
</code></pre></div></div> <ul> <li>Setting Up Automatic Certificate Renewal<br/> This certificate need to renewal every 3 months.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>crontab <span class="nt">-e</span>
</code></pre></div></div> <p>It will show nano editor and add the following to set up certificate renewal automatically.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 3 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> certbot renew <span class="nt">--quiet</span> <span class="o">&amp;&amp;</span> systemctl restart nginx
</code></pre></div></div> <p>After edit control+X -&gt; y -&gt; enter (Mac user)</p> <p>Test certificate renewal</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>certbot renew <span class="nt">--dry-run</span>
</code></pre></div></div> <p>If the message shows “Congratulations, all simulated renewals succeeded”, then it is working properly.</p> <h1 id="download-docker-and-build-the-project">Download Docker and build the project</h1> <h2 id="install-docker">Install Docker</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> docker
<span class="nb">sudo </span>systemctl start docker
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>docker
</code></pre></div></div> <h2 id="setting-docker-permission">Setting Docker permission</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker ec2-user
newgrp docker
</code></pre></div></div> <p>After doing this, everytime when you call the docker command, you do not need to add ‘sudo’. Need to exit the EC2 terminal and re-enter the EC2 terminal to apply it.</p> <h2 id="build-docker-image-and-implement">Build Docker image and implement</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>your-repo
docker build <span class="nt">-t</span> image-name <span class="nb">.</span>
</code></pre></div></div> <p>Implement</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-d</span> <span class="nt">--name</span> image-name-container <span class="nt">--env-file</span> .env <span class="nt">-p</span> port#:port# image-name
</code></pre></div></div> <ul> <li>“-d”: Run the container in the background</li> </ul> <h2 id="check-the-container-status">Check the container status</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker ps
</code></pre></div></div> <h1 id="setting-auto-update">Setting Auto-update</h1> <p>Write the script to automatically update when the project code has been changed.</p> <h2 id="creating-an-automatic-update-script">Creating an Automatic Update Script</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano /home/ec2-user/your-repo/auto_deploy.sh
</code></pre></div></div> <p>Then add following content.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="nb">cd</span> /home/ec2-user/your-repo

<span class="c"># Pull the newest code from Git</span>
git pull origin main

<span class="c"># Stop and remove Docker container</span>
docker stop image-name-container
docker <span class="nb">rm </span>image-name-container

<span class="c"># Build and implement new image</span>
docker build <span class="nt">-t</span> image-name <span class="nb">.</span>
docker run <span class="nt">-d</span> <span class="nt">--name</span> image-name-container <span class="nt">--env-file</span> .env <span class="nt">-p</span> port#:port# image-name
</code></pre></div></div> <p>control + X -&gt; Y -&gt; Enter to save it.</p> <h2 id="grant-execution-permissions">Grant Execution Permissions</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod</span> +x /home/ec2-user/your-repo/auto_deploy.sh
</code></pre></div></div> <h2 id="add-auto-update-to-crontab">Add auto-update to Crontab</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>crontab <span class="nt">-e</span>
</code></pre></div></div> <p>Add the following.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> /home/ec2-user/your-repo/auto_deploy.sh <span class="o">&gt;&gt;</span> /home/ec2-user/your-repo/deploy.log 2&gt;&amp;1
</code></pre></div></div> <p>Save and close it.<br/> It will check your ropository to update every an hour.</p> <p>Done. Now check ‘http://your-domain.com’ and ‘https://your-domain.com’</p>]]></content><author><name></name></author><category term="Cloud_Computing"/><category term="WIS"/><summary type="html"><![CDATA[Processing of deployment with AWS EC2 and Docker]]></summary></entry><entry><title type="html">Decision Tree</title><link href="https://namdarine.github.io/blog/2024/Decision-Tree/" rel="alternate" type="text/html" title="Decision Tree"/><published>2024-12-25T18:00:00+00:00</published><updated>2024-12-25T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2024/Decision-Tree</id><content type="html" xml:base="https://namdarine.github.io/blog/2024/Decision-Tree/"><![CDATA[<p>A decision tree is a flowchart-like structure used to make decision or prediction. It is a decision support recursive partitioning structure that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resources costs, and utility.</p> <p>The decision tree is a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.</p> <h2 id="structure">Structure</h2> <p><strong>Nodes</strong>: represent decisions or tests on attributes.</p> <ul> <li><strong>Root node</strong>: represents the entire dataset and the initial decision to be made. - <strong>Internal node</strong>: represents decisions or tests on attributes. Each internal node has one or more branches.</li> <li><strong>Leaf node</strong>: represents the final decision or prediction. No further splits occur at these nodes.</li> </ul> <p><strong>Branche</strong>: represents the outcome of a decision or test, leading to another node.</p> <h2 id="metrics">Metrics</h2> <ol> <li> <p><strong>Gini impurity</strong><br/> Measure the likelihood that a randomly classified new instance will be misclassified based on the class distribution of the dataset.</p> \[Gini = 1 - \sum_{i = 1}^{n} {p_i^{2}}\] <p>where \(p_i\) is the probability of an instance being classified into a particular class.</p> </li> <li> <p><strong>Entropy</strong><br/> Measures the amount of uncertainty or impurity in the dataset.</p> \[entropy = -\sum_{i = 1}^{n} {p_i \log_2{p_i}}\] <p>where \(p_i\) is the probability of an instance being classified into a particular class.</p> </li> <li> <p><strong>Information gain</strong><br/> Measures the reduction in entropy or Gini impurity after a dataset is split on an attribute.</p> </li> </ol> <h2 id="advantages">Advantages</h2> <ol> <li> <p><strong>Simplicity and Interpretability</strong></p> <ul> <li>Easy to understand and interpret</li> <li>Visual representation mirrors human decision-making processes</li> <li>White-box model: Conditions and results can be explained using boolean logic</li> </ul> </li> <li> <p><strong>Minimal Data Preparation</strong></p> <ul> <li>No need for normalization or scaling</li> <li>No need for dummy variables</li> <li>Can handle missing values (depending on the algorithm)</li> </ul> </li> <li> <p><strong>Versatile</strong></p> <ul> <li>Suitable for both <u>classification and regression</u> tasks</li> <li>Can handle numerical and catagorical data</li> <li>supports multi-output problems</li> </ul> </li> <li> <p><strong>Efficiency</strong></p> <ul> <li>Prediction cost is logarithmic in the number of training data points</li> </ul> </li> <li> <p><strong>Robustness</strong></p> <ul> <li>Performs well even if assumptions are somewhat violated</li> </ul> </li> <li> <p><strong>Validation-Friendly</strong></p> <ul> <li>Models can be validated using statistical tests, ensuring reliability</li> </ul> </li> <li> <p><strong>Non-linear Relationships</strong></p> <ul> <li>Capable of capturing complex, non-linear relationships between features and target variables</li> </ul> </li> </ol> <h2 id="disadvantages">Disadvantages</h2> <ol> <li> <p><strong>Overfitting</strong></p> <ul> <li>Trees can become overly complex and fail to generalize well</li> <li>Mitigation: <u>Pruning,</u> setting minimum samples per leaf node, or maximum tree depth</li> </ul> </li> <li> <p><strong>Instability</strong></p> <ul> <li>Small changes in data can lead to completely different tree structures</li> <li>Mitigation: Use ensembel methods, such as Random Forest, Gradient Boosting</li> </ul> </li> <li> <p><strong>Limited Extrapolation</strong></p> <ul> <li>Predictions are piecewise constant and not smooth</li> <li>Poor performance in extrapolating beyond the training data range</li> </ul> </li> <li> <p><strong>Suboptimal Solutions</strong></p> <ul> <li>Finding the optimal decision tree is NP-complete</li> <li>Algorithms rely on <u>heuristic methods</u>, such as greedy algorithms</li> <li>Mitigation: Train multiple trees using ensemble methods</li> </ul> </li> <li> <p><strong>difficulty with Certain Patterns</strong></p> <ul> <li>Struggles with learning patterns like XOR, parity, or multiplexer problems</li> </ul> </li> <li> <p><strong>Bias Toward Dominant Classes</strong></p> <ul> <li>Decision trees may become biased if certain classes dominate the dataset</li> <li>Mitigation: Ensure a balanced dataset before training</li> </ul> </li> <li> <p><strong>Feature Bias</strong></p> <ul> <li>Features with more levels/categories may dominate the tree structure</li> </ul> </li> </ol> <h2 id="application">Application</h2> <ul> <li>Business Decision Making</li> <li>Healthcare</li> <li>Finance</li> <li>Marketing</li> </ul> <h2 id="reference">Reference</h2> <ul> <li>“1.10. Decision Trees.” Scikit, scikit-learn.org/1.5/modules/tree.html. Accessed 30 Dec. 2024.</li> <li>“Decision Tree.” Wikipedia, Wikimedia Foundation, 20 Oct. 2024, en.wikipedia.org/wiki/Decision_tree.</li> <li>“Decision Tree.” GeeksforGeeks, 17 May 2024, www.geeksforgeeks.org/decision-tree/.</li> </ul>]]></content><author><name></name></author><category term="AI/ML"/><category term="WIL"/><summary type="html"><![CDATA[Understand the concept of the Decision Tree]]></summary></entry><entry><title type="html">Naive Bayes Classifier</title><link href="https://namdarine.github.io/blog/2024/Naive-Bayes/" rel="alternate" type="text/html" title="Naive Bayes Classifier"/><published>2024-12-20T18:00:00+00:00</published><updated>2024-12-20T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2024/Naive-Bayes</id><content type="html" xml:base="https://namdarine.github.io/blog/2024/Naive-Bayes/"><![CDATA[<blockquote> <p>Naive Bayes classifier is a family of linear <strong>Probabilistic classifier</strong>.</p> </blockquote> <p>-&gt; Assumes the features are conditionally independent given the target.</p> <p>This classifier is one of the types of the simplest Bayesian network models.<br/> Also, it is widely utilized for its simplicity and efficiency in machine learning.<br/> As it is the family of “linear” probabilistic classifier, it is highly scalable, requiring a number of parameters linear in the number of varibales in a learning problem.<br/> Naive Bayes classifier is mostly used in the language model. It is fast and making prediction is easy with high dimension of data. The naive Bayes classifier is based on Bayes’ Theorem, \(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\), where A and B are events and \(P(B) \neq 0\).<br/> For more information on Bayes’ Theorem, check <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Wikipedia</a>.</p> <h2 id="assumption">Assumption</h2> <ul> <li><strong>Feature independence</strong>: The features of the data are conditionally independent of each other.</li> <li><strong>Continuous features are normally distributed</strong>: If a feature is continuous, then it is assumed to be normally distributed within each class.</li> <li><strong>Discrete features have multinomial distributions</strong>: If a feature is discrete, then it is assumed to have a multinomial distribution within each class.</li> <li><strong>Features are equally important</strong>: All features are assumed to contribute equally to the prediction of the class label.</li> <li><strong>No missing data</strong>: The data should not contain any missing data.</li> </ul> <h2 id="advantages">Advantages</h2> <ul> <li>Easy to implement and computationally efficient.</li> <li>Effective in cases with a large number of features.</li> <li>Performs well even with limited training data.</li> <li>Performs well in the presence of categorical features.</li> <li>For numerical features data is assumed to come from normal distributions.</li> </ul> <h2 id="disadvantages">Disadvantages</h2> <ul> <li>Assumes that features are independent, which may not always hold in real-world data.</li> <li>Can be influenced by irrelevant attributes.</li> <li>May assign zero probability to unseen events, leading to poor generalization.</li> </ul> <h2 id="applications">Applications</h2> <ul> <li><strong>Spam email filtering</strong></li> <li><strong>Text classification</strong>: used in sentiment analysis, document categorization, and topic classification.</li> <li><strong>Medical diagnosis</strong>: helps in predicting the likelihood of a disease based on symptoms.</li> <li><strong>Credit scoring</strong>: Evaluates the creditworthiness of individuals for loan approval.</li> <li><strong>Weather prediction</strong></li> </ul> <h1 id="referrence">Referrence</h1> <ul> <li>CS 481, Artificial Intelligence Language Understanding, prof. Jacek Dzikowski, Spring 2024, Illinois Tech</li> <li>Comment, et al. “Naive Bayes Classifiers.” GeeksforGeeks, 10 July 2024, www.geeksforgeeks.org/naive-bayes-classifiers/.</li> <li>“Naive Bayes Classifier.” Wikipedia, Wikimedia Foundation, 28 Nov. 2024, en.wikipedia.org/wiki/Naive_Bayes_classifier.</li> </ul>]]></content><author><name></name></author><category term="AI/ML"/><category term="WIL"/><summary type="html"><![CDATA[Understand the concept of the Naive Bayes Classifier]]></summary></entry><entry><title type="html">k-Means Clustering</title><link href="https://namdarine.github.io/blog/2024/kMeansClustering/" rel="alternate" type="text/html" title="k-Means Clustering"/><published>2024-12-17T18:00:00+00:00</published><updated>2024-12-17T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2024/kMeansClustering</id><content type="html" xml:base="https://namdarine.github.io/blog/2024/kMeansClustering/"><![CDATA[<h1 id="what-is-k-means-clustering">What is K-means Clustering</h1> <p>The k-Means algorithm is one of the most popular iterative descent clustering methods and the unsupervised learning’ algorithm. K stands for clustering which assigns data points to one of the K clusters based on the distance from the cluster center. Start by randomly assigning the cluster center to the space. Each data point is then assigned to one of the clusters based on the distance from the cluster center. Assign each point to one of the clusters and assign a new cluster center. This process runs repeatedly until a good cluster is found. Should assign points to one of the groups assuming that the number of clusters is given in advance in the analysis.<br/> It aim for situations where all variables are quantitative types, and select Euclidean distance squared as a measure of dissimilarity.</p> \[d(x_{i}, x_{i'}) = \sum_{j=1}^{p} {(x_{ij} - x_{i'j})^2} = \|\|{x_{i} - x_{i'}}\|\|^2\] <p>In some cases, K is not clearly defined, and we have to think about the optimal number of K. K means clustering performs best when data is well separated. When data points overlap, this clustering is not suitable. To get help defining the K, we can use the ‘Silhouette score’ or the ‘Elbow method’ The criterion is minimized by allocating N observations to K clusters in such a way that the average difference from the cluster mean defined by the points of theat cluster within each cluster is minimized.</p> <ul> <li><strong>Unsupervised Learning</strong>: Uses machine learning to analyze unlabeled datasets to discover patterns without human supervision.</li> </ul> <h1 id="goal-of-clustering">Goal of Clustering</h1> <p>The goal of clustering is to divide a population or set of data points into groups so that data points within each group are more similar to each other and differ from data points within other groups. Clusters are essentially grouping objects according to how similar and different they are from each other and discover underlying patterns or structures within the data.</p> <h1 id="algorithm">Algorithm</h1> <ol> <li>For a given cluster assignment \(C\), the total cluster variance, \(\min_{C, {m_{k}}_{1}^{K}} {\sum_{k=1}^{K} {N_{k}} \sum_{C(i)=k} {\|\|x_i - m_k\|\|^2}}\), is minimized with respect to \({m_1, \cdots, m_K}\) yiedling the means of the currently assigned clusters, \(\bar{x*S} = \argmin*{m} {\sum\_{i \in S} {\|\|x_i - m\|\|^2}}\).</li> <li>Given a current set of means \({m*{1}, \cdots, m*{K}}\), \(\min*{C, {m*{k}}_{1}^{K}} {\sum_{k=1}^{K} {N*{k}} \sum* {\|\|x_i - m_k\|\|^2}}\) is minimized by assigning each observation to the closest cluster means. \(C(i) = \argmin\_1 {\|\|x_i - m_k\|\|^2}\)</li> <li>Steps 1 and 2 are iterated until the assignments do not change.</li> </ol> <p>Each of the first and second steps reduces the value of the criterion to ensure convergence. However, the results may indicate a suboptimal local minimum. In addition, start the algorithm with various random choices for the starting means, and we need to select the solution with the smallest value of the objective function.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/k-means-480.webp 480w,/assets/img/k-means-800.webp 800w,/assets/img/k-means-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/k-means.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="AI/ML"/><category term="WIL"/><summary type="html"><![CDATA[Understand the concept of the k-Means Clustering]]></summary></entry><entry><title type="html">Machine Learning Final Preparation</title><link href="https://namdarine.github.io/blog/2024/ML-Final_Prep/" rel="alternate" type="text/html" title="Machine Learning Final Preparation"/><published>2024-11-27T18:00:00+00:00</published><updated>2024-11-27T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2024/ML-Final_Prep</id><content type="html" xml:base="https://namdarine.github.io/blog/2024/ML-Final_Prep/"><![CDATA[<h2 id="what-is-the-condition-number-of-a-matrix-and-why-is-it-important-in-machine-learning">What is the condition number of a matrix and why is it important in Machine Learning?</h2> <ul> <li>The state of regulation of a matrix can be shown from its conditional number. In this case, a measure of how sensitive the solution to a linear equation is to the data changes or changes to the matrix itself.</li> </ul> <p>Condition numbers are, thus, important to understand in machine learning because they make our models unstable and, even worse, very sensitive and bad, in cases of using a regression training algorithm for our linear models.</p> <p>This means that a small perturbation or error may cause significant changes in the output for the same inputs. This is especially alarming in machine learning, which develops models using noisy or incomplete data. Model predictions can be negatively impacted by high sensitivity and may yield unreliable results.</p> <p>The condition number is not only significant for solution precision, but it is also paramount for the generalization of machine learning algorithms. Dress a matrix in a condition number that means that it is well conditioned, and you can have a much smoother landscape with a better optimization convergence during training and have models that generalize effectively to unseen data. On the other hand, under-conditioned models are likely to overfit the training data, which is a characteristic of lower generalization ability.</p> <h3 id="citation">Citation</h3> <ul> <li>Guggenheimer, H., Edelman, A., \&amp; Johnson, C. R. (1995). A Simple Estimate of the Condition Number of a Linear System. College Mathematics Journal. https://www.tandfonline.com/doi/epdf/10.1080/07468342.1995.11973657?needAccess=true</li> </ul> <h2 id="what-are-lasso-and-ridge-regularization-when-do-you-need-to-use-regularization-would-you-use-lasso-or-ridge-in-a-high-dimensional-problem-which-is-generally-more-accurate-why">What are LASSO and Ridge regularization? When do you need to use regularization? Would you use LASSO or Ridge in a high-dimensional problem? Which is generally more accurate? Why?</h2> <p>LASSO: A regression analysis method that enhances the prediction accuracy and interpretability of the resulting statistical model by performing both variable selection and regularization. Lasso penalizes based on the absolute value of the coefficients. It not only shrinks the coefficient but also some can be set to exactly zero, making it a variable selection technique.</p> <p>Ridge regularization: Adds a penalty equivalent to the square of the magnitude of coefficients. In this case, use an L2 norm of the coefficients, which shrinks all coefficients towards zero, but does not eliminate them. Additionally, Rdge performs better when the multicollinearity among the predictors is present.</p> <p>Using regularization is useful for high-dimensional datasets since it curbs excessive fitting of those datasets.</p> <p>I would use Lasso. It is also common practice to use Lasso in some cases due to its variable selection ability. It can shrink the model incrementally and eventually force some coefficients to be exactly zero. This could make the model simpler and more interpretable, possibly enhancing its performance in the presence of new data. However, Ridge, most of the time, incorporates all predictors, which might not be suitable in a situation when one is trying to build a prediction from the most crucial features.</p> <p>In essence, Ridge estimates tend to be better when there is high multicollinearity while all the features of the data are of interest because it yields more stable estimates in this case. To handle this concern, Ridge reduces the size of the coefficients and retains all the features thereby increasing the robustness of the analysis and the results.</p> <h3 id="citation-1">Citation</h3> <p>-Ranstam, J., Cook, J.A. (2018). LASSO regression. British Journal of Surgery, 105.</p> <h2 id="under-what-conditions-are-k-fold-cross-validation-and-bootstrapping-a-poor-choice-for-model-selection-what-are-some-alternatives-in-these-cases">Under what conditions are k-fold cross validation and bootstrapping a poor choice for model selection? What are some alternatives in these cases?</h2> <ul> <li> <h3 id="k-fold-cross-validation">k-fold cross-validation:</h3> </li> </ul> <ol> <li> <p>High-dimensional settings where predictors are independent of class labels but the model is allowed to select predictors, cross-validation may give misleadingly optimistic results if feature selection is not properly handled within the cross-validation process.</p> <ul> <li>Alternatives: Lasso / Ridge Regularization, PCA</li> </ul> </li> <li> <p>When there is considerable variability in the error estimates. Error estimates obtained from CV can show significant variability depending on how the data is split. Relying solely on the mean performance may lead to overestimating the model’s reliability.</p> <ul> <li>Alternatives: Repeated cross-validation, Bootstrap</li> </ul> </li> <li> <p>If the model is not completely retrained for each fold, including any feature selection or preprocessing steps. This is a common blunder even in top-ranked journal papers.</p> <ul> <li>Alternatives: Nested cross-validation</li> </ul> </li> <li> <p>When working with large numbers of predictors, such as in genomics, improper cross-validation implementation can have dramatic consequences in terms of incorrect error estimation.</p> <ul> <li>Alternatives: Regularization, Feature Filtering, Nested CV, PCA</li> </ul> </li> </ol> <ul> <li> <h3 id="bootstrapping">Bootstrapping</h3> </li> </ul> <ol> <li> <p>Unsupervised learning, such as clustering, because cluster centers will tend to fill feature space densely and be close to all data points regardless of K.</p> <ul> <li>Alternatives: cross-validation, AIC/BIC, Silhouette score, or elbow</li> </ul> </li> <li> <p>Leave-One-Out (K = N) Bootstrap. When K = N, where every data point is included except one. It is low bias and high variance. The training sets are almost identical across iterations, leading to a highly variable estimate despite low bias. Also, it has a high computational cost. Running N iterations can become computationally expensive.</p> <ul> <li>Alternatives: k-fold cross-validation</li> </ul> </li> <li> <p>The bootstrap error estimate does not provide a good estimate in general.</p> <ul> <li>Alternatives: cross-validation, AIC/BIC, Ensemble evaluaion</li> </ul> </li> <li> <p>Problems with standard errors that may be invalid due to the influence of the model selection process and optimization criteria after model selection that may differ from the desired criteria on the training set.</p> <ul> <li>Alternatives: Cross-validation, AIC/BIC</li> </ul> </li> <li> <p>Problems with traditional methods, such as F-statistics, that adding or removing terms based on significance fails to account for multiple testing issues adequately.</p> <ul> <li>Alternatives: AIC/BIC, cross-validation, Regularization (Lasso and Ridge)</li> </ul> </li> </ol> <h2 id="citation-2">Citation</h2> <ul> <li>Trevor Hastie, Robert Tibshirani, and Jerome Friedman, 2009, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition), Ch 7 : Model Assessment and Selection</li> </ul> <h2 id="give-a-brief-descrption-of-dropout-why-is-it-used-so-widely-in-deep-learning">Give a brief descrption of dropout. Why is it used so widely in Deep Learning?</h2> <ul> <li>To address overfitting, Dropout randomly sets a subset of neurons to zero at the start of each training cycle. This procedure increases robustness by preventing complex co-adaptations and forcing the network to learn redundant representations. Dropout, by its nature, makes the network less sensitive to individual neurons. As a result, this model can generalize and perform better on new, untested data. A more generalized representation that can manage different inputs is achieved using dropout, allowing each model update to be viewed as a mini-semble of multiple neural network topologies. It is also shown to improve performance on a wide range of tasks and datasets. Finally, it helps the model perform better and train faster. During training, it allows models to learn more efficiently and reach their best performance quicker by temporarily turning off some neurons during training.</li> </ul> <h3 id="citation-3">Citation</h3> <ul> <li>Byrd, J., \&amp; Lipton, Z. C. (2018). What is the Effect of Importance Weighting in Deep Learning? International Conference on Machine Learning. https://www.semanticscholar.org/paper/b661520bf0061b7d96ccf12016e351dd3a6ee780</li> </ul> <h2 id="why-are-convolutional-layers-used-widely-in-image-classification-and-segmentation">Why are convolutional layers used widely in image classification and segmentation?</h2> <ul> <li>There are several reasons. First, it is utilized for an explicit feature extraction, which is localized in a region of interest. Such features are derived from local areas of the image, which enables the identification of edges, textures, and other low-level patterns. Second, these local features help in enhancing the correlation as well as the intensity of pixels from areas that are closer as opposed to those that are farther away. It is important to mention that during the image classification process, cure areas are concentrated, which makes local features even more significant.</li> </ul> <p>Second, it is utilized for the purpose of architecture that is economical. For instance, architecture that has been designed with weight sharing in mind for a structure such as a neural network can apply the same filters or kernels all over the entire image, thus minimizing the number of parameters, as well as the number of computations required to be performed. Such relevant units in the feature maps process little portions at a time while interrelating and distinguishing a pattern.</p> <p>Third, it is utilized for the hierarchical feature processing. Convolutional networks build hierarchical representations, starting with simple features and progressing to more complex patterns. In the same way, the local features detect some parts, which in the next stage get combined for general feature detection. Moreover, deep representations are less affected than shallow representations by transformations like translations, scaling, and small rotation of the images.</p> <p>Fourth, it is used in the context of invariance. Convolutional layers give consistent regions as patterns that can be recognized. Such patterns can also be enhanced through data augmentation or more sophisticated architectures to be invariant to scaling or rotations.</p> <p>Fifth, it is used for efficient learning of features and structures of patterns as well as deeper networks. Convolutional networks make use of local receptive fields, weight sharing, and subsampling to hit efficiently, and with so many processes, the image across features. Earlier networks with no invariances had average higher misclassification rates of around 4.5\%, but these architecture limitations have largely been alleviated by complementary ones other architectures have strengthened.</p> <p>Lastly, emphasizes other methods as wavelet transforms. In some of the tasks, wavelet transforms are used in conjunction with CNNs in order to increase edge detection and capture discrete jumps in images, although CNNs are now robust enough to perform such tasks independently.</p> <h2 id="citation-4">Citation</h2> <ul> <li>Bishop, C. M. (2006). Chapter 5: Neural networks. In Pattern recognition and machine learning. Springer.</li> <li>Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), Ch 11: Neural Networks, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition)</li> </ul> <h2 id="if-model-a-gives-a-rmse-of-032-and-model-b-gives-a-rmse-of-034-same-training-and-out-of-sample-validation-sets-what-are-some-factors-you-would-consider-in-determining-which-is-the-better-model-why-are-these-factors-relevant">If Model A gives a RMSE of 0.32 and Model B gives a RMSE of 0.34 (same training and out-of-sample validation sets), what are some factors you would consider in determining which is the better model? Why are these factors relevant?</h2> <ul> <li>There are several factors to consider in determining which model is the better. First, model Complexity and interpretability. If the model is too complicated, then the risk of overfitting increases. The complex model has good performance in the train data, but it might not perform well in the new data. Also, too complex a model is hard to interpret, and there are possible debugging difficulties. The simple model makes it easy to understand the result or the processing of the prediction.</li> </ul> <p>Second, robustness to overfitting. If the model is overfitting to the training data, it does not generalize well to the new data. In particular, if the training data is low or noisy, the model is likely to overfit. Especially with less training data or too much noise, there is the possibility to be overfitting.</p> <p>Lastly, training time and resource efficiency. A more complicated model takes more time to train, and the time required to train models can increase exponentially, especially when trained on a large dataset. Also, a complex model such as a deep learning model can require significant computational resources.</p> <h3 id="citation-5">Citation</h3> <ul> <li>Bishop, C. M. (2006). Chapter 3: Linear Models for Regression. In Pattern recognition and machine learning. Springer.</li> <li>Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), Ch 7: Model Assessment and Selection, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition)</li> <li>Goodfellow, I., Bengio, Y., \&amp; Courville, A. (2016). Chapter 5: Machine learning basics. In Deep learning. MIT Press. Retrieved from http://www.deeplearningbook.org</li> </ul> <h2 id="you-are-given-some-binary-classification-training-data-with-two-features-x_1-x_2-both-x_1-and-x_2-are-0-1-valued-you-can-easily-calculate-px1y-y-and-px2y-y-and-py-y-for-y-0-1-but-what-else-do-you-need-to-check-in-order-to-satisfy-the-conditions-for-using-a-naive-bayes-classifier">You are given some binary classification training data with two features $X_1$, $X_2$. Both $X_1$ and $X_2$ are {0, 1}-valued. You can easily calculate $P(X1|Y= y)$ and $P(X2|Y= y)$ and $P(Y= y)$ for $y= 0, 1$, but what else do you need to check in order to satisfy the conditions for using a Naive Bayes classifier?</h2> <ul> <li> <table> <tbody> <tr> <td>We need to check the conditional independence of features given the class, $P(X_1, X_2</td> <td>Y = y) = P(X_1</td> <td>Y = y) \cdot P(X_2</td> <td>Y = y)$ and sufficient data, enough data to accurately estimate the conditional probabilities.</td> </tr> </tbody> </table> </li> </ul> <h3 id="citation-6">Citation</h3> <ul> <li>Bishop, C. M. (2006). Chapter 4: Linear Models for Classification. In Pattern recognition and machine learning. Springer.</li> </ul> <h2 id="are-association-rules-as-found-by-the-a-priori-algorithm-transitive-eg-if-a-rightarrow-b-and-b-rightarrow-c-does-a-rightarrow-c">Are association rules as found by the a priori algorithm transitive? (e.g. if $A \Rightarrow B$ and $B \Rightarrow C$, does $A \Rightarrow C$?)</h2> <ul> <li>No. It does not guarantee that $ A \Rightarrow C$ will also be valid if $A \Rightarrow B$ and $B \Rightarrow C$ are valid association rules. Because the strength of the association rule is determined by its support and confidence, but because $A \Rightarrow B$ and $B \Rightarrow C$ hold with high support and confidence does not imply that $ A \Rightarrow C$ will also hold with high support and confidence. The Apriori algorithm generates rules based on empirical patterns found in the data. The relationship between A and C is not guaranteed by the relationship found in $A \Rightarrow B$ or $B \Rightarrow C$. The association rules, however, rely on data rather than logical reasoning which may lead to no establishment of transferability.</li> </ul> <h3 id="citation-7">Citation</h3> <ul> <li>Tan, P.-N., Steinbach, M., &amp; Kumar, V. (2021). Chapter 5: Association analysis: Basic concepts and algorithms. In Introduction to data mining (2nd ed.). Pearson.</li> <li>Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), Ch 14: Unsupervised Learning, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition)</li> </ul> <h2 id="is-the-following-statement-true-or-false-performing-pca-before-utilizing-another-classification-or-regression-model-can-reduce-the-number-of-input-features-and-improve-performance-and-therefore-is-a-good-idea-if-true-provide-some-supporting-details-if-false-give-a-counterexample">Is the following statement true or false? “Performing PCA before utilizing another classification or regression model can reduce the number of input features and improve performance and therefore is a good idea.” If true, provide some supporting details. If false, give a counterexample.</h2> <ul> <li>False. PCA captures linear correlations, so it may fail to preserve important nonlinear relationships in the data. Applying PCA can remove informative features, reducing the performance of the classifier or regressor. Also, PCA does not consider the label. PCA is the unsupervised method that does not make use of any labels in the computation. Therefore, if the classification or regression model is the supervised model, such as logistic regression and decision trees, then performing PCA before utilizing another model may not improve performance.</li> </ul> <h3 id="citation-8">Citation</h3> <ul> <li>Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), Ch 3: Linear Methods for Regression, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition)</li> </ul> <h2 id="is-the-following-statement-true-or-false-performing-k-means-and-then-adding-labels-of-the-resulting-clusters-to-your-set-of-features-can-enhance-your-feature-set-and-improve-your-classificationregression-model-and-is-therefore-a-good-idea-if-true-provide-some-supporting-details-if-false-give-a-counterexample">Is the following statement true or false? “Performing k-means and then adding labels of the resulting clusters to your set of features can enhance your feature set and improve your classification/regression model and is therefore a good idea.” If true, provide some supporting details. If false, give a counterexample.</h2> <ul> <li>False. The label generated by k-means clustering and the actual label may not match. Then the clustered label may act as noise because it is not useful information. In addition, the feature space may be unnecessarily expanded. Small data and large number of clusters can lead to overfitting.</li> </ul> <h3 id="citation-9">Citation</h3> <ul> <li>Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), Ch 14: Unsupervised Learning, The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Second Edition)</li> </ul>]]></content><author><name></name></author><category term="AI/ML"/><category term="WIL"/><summary type="html"><![CDATA[Answering the questions to prepare the final]]></summary></entry><entry><title type="html">Security - Attacks</title><link href="https://namdarine.github.io/blog/2024/Security-attacks/" rel="alternate" type="text/html" title="Security - Attacks"/><published>2024-09-22T18:00:00+00:00</published><updated>2024-09-22T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2024/Security-attacks</id><content type="html" xml:base="https://namdarine.github.io/blog/2024/Security-attacks/"><![CDATA[<p>There are two different types of attacks, passive attack and active attack.</p> <h1 id="passive-attack">Passive Attack</h1> <blockquote> <p>Attempt to learn or make use of information, but not affect system resources.</p> </blockquote> <p>Such as, release message contents and traffic analysis.<br/> It is relatively hard to detect, but easier to prevent by encryption.</p> <h2 id="realease-message-contents">Realease Message Contents</h2> <p>Message content is one of the passive attack. It involves the intruder stealing all the message or data transmitted. The information gathered by the intruder is stolen unethically.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Security/release_message-480.webp 480w,/assets/img/Security/release_message-800.webp 800w,/assets/img/Security/release_message-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Security/release_message.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="traffic-analysis">Traffic Analysis</h2> <p>Masked traffic analysis is also one of the passive attack. It involves messages or data eing encrypted before transmission. The message being masked the intruder can’t read the message but only understand the pattern and length of encryption.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Security/traffic_analysis-480.webp 480w,/assets/img/Security/traffic_analysis-800.webp 800w,/assets/img/Security/traffic_analysis-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Security/traffic_analysis.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="active-attack">Active Attack</h1> <blockquote> <p>Attempt to alter system resources or affect their operation.</p> </blockquote> <p>It is relatively hard to prevent because it would require physical protection of all communications facilities and paths at all times, but easier to detect.</p> <p>There are four categories.</p> <ul> <li>Maquerade</li> <li>Replay</li> <li>Modification of messages</li> <li>Denial of service</li> </ul> <h2 id="masquerade-attack">Masquerade Attack</h2> <p>The attacker tampers the information received by the receiver by claiming itself as the sender. It takes place when one entity pretends to be a different entity. It includes one of the other forms of active attack.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Security/masquerade_attack-480.webp 480w,/assets/img/Security/masquerade_attack-800.webp 800w,/assets/img/Security/masquerade_attack-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Security/masquerade_attack.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="replay-attack">Replay Attack</h2> <p>The attacker attacks the transmitted message through a passive channel and make the final message received by the receiver may appear to be authorized and safe. It involves the passive capture of a previously transmitted message and replaying it to produce an unauthorized effect.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Security/replay-480.webp 480w,/assets/img/Security/replay-800.webp 800w,/assets/img/Security/replay-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Security/replay.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="modification-attack">Modification Attack</h2> <p>Some portion of a legitimate message is altered, or messages are delayed or reordered to produce an unauthorized effect.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Security/modification-480.webp 480w,/assets/img/Security/modification-800.webp 800w,/assets/img/Security/modification-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Security/modification.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="denial-of-service-attack">Denial of Service Attack</h2> <p>The receiver is prevented from receiving the transmitted message as there is an overflow of requests to the receiver, which makes the services hampered from their usual behavior.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Security/server-480.webp 480w,/assets/img/Security/server-800.webp 800w,/assets/img/Security/server-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Security/server.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="Info_Security"/><category term="WIL"/><summary type="html"><![CDATA[Concepts and understanding of Information Security]]></summary></entry><entry><title type="html">Security Policy</title><link href="https://namdarine.github.io/blog/2024/Security-Security_Policy/" rel="alternate" type="text/html" title="Security Policy"/><published>2024-09-20T18:00:00+00:00</published><updated>2024-09-20T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2024/Security-Security_Policy</id><content type="html" xml:base="https://namdarine.github.io/blog/2024/Security-Security_Policy/"><![CDATA[<blockquote> <p>A set of rules and practices that specify how a system or organization provides security services to protect its assets.</p> </blockquote> <p>Security policy defines the measures that are in place to <u>ensure the confidentiality, integrity, and availability</u> of data, networks, and system.</p> <h1 id="things-covered">Things covered</h1> <ul> <li><strong>Who can access what</strong>: explain who is allowed to use certain computers, programs, and information</li> <li><strong>How to keep data safe</strong>: provide steps for keeping data safe from being stolen or lost</li> <li><strong>What to do if something goes wrong</strong>: give instructions on how to handle problems like computer hacks, data breaches, or other security issues</li> <li><strong>Using the internet and emails safely</strong>: provide guidelines for using the internet and emails in a way that doesn’t put the organization at risk</li> <li><strong>Making sure software is up to date</strong>: explain why it’s important to regularly update software and how to do it</li> <li><strong>Training employees</strong>: include instructions for teaching employees about security risks and how to protect the organization</li> <li><strong>Physical security</strong>: might cover things like locking doors, keeping servers in safe places, and making sure only authorized people can enter certain areas</li> </ul> <h1 id="aspects-of-security">Aspects of Security</h1> <blockquote> <p>The security architecture provides a systematic approach to address the dynamic and evolving nature of security challenges in information systems.</p> </blockquote> <p>It offers a framework for organizations to build and maintain a secure environment for their <u>data, systems, and networks</u> by focusing on attacks, mechanisms, and services.</p> <h2 id="security-attack">Security Attack</h2> <p>These attacks can take <strong>various forms</strong>, unauthorized access, disclosure, modification, or destruction of data, systems, or networks.<br/> The goal is to protect against such actions and maintain the confidentiality, integrity, and availability of information (CIA Traids).</p> <h2 id="security-mechanism">Security Mechanism</h2> <p>Methods designed to detect, prevent, or recover from security attacks. They act as security barriers to safeguard an organization’s assets. They operate independently or in conjunction.</p> <h3 id="common-security-mechanisms">Common security mechanisms</h3> <ul> <li>Cryptography</li> <li>Message digests and digital signatures</li> <li>Digital certificates</li> <li>Public Key Infrastructure (PKI): A framework that manages digital keys and certificates, facilitating secure communication. &amp;nbsp Different entieties excange information using digital certification</li> </ul> <h2 id="security-service">Security Service</h2> <p>It is functionalities offered by security mechanisms to enhance the security of a system. And it represent the specific functionalities offered by security mechanisms. It can include authentication, access control, data confidentiality, data integrity, and non-repudiation.</p> <h3 id="authentication">Authentication</h3> <blockquote> <p>Used to assure the identity of the sender or creator of the data. It is a process of verifying the identity of a user, device, or system, to ensure that it is not and imposter or malicious actor.</p> </blockquote> <p>=&gt; Assure that the communicating entity is the one that it claims to be</p> <h4 id="two-specific-authentication-services">Two specific authentication services</h4> <ul> <li><strong>Peer Entity Authentication</strong>: Used to authenticate the identity of other entities with which the system is communicating</li> <li><strong>Data Origin Authentication</strong>: Used to authenticate the origin of a message or data, to ensure that it was sent by the entity that it claims to be form</li> </ul> <p>Both of these services are important for ensuring the authenticity and integrity of communication, in order to prevent unauthorized access, tampering, or impersonation. It ensures that the data or communication is from the intended source and not an imposter or an attacker.</p> <p><strong>Process</strong>: Identification -&gt; Authentication -&gt; Authorization -&gt; Auditing -&gt; Accountability -&gt; IAAA</p> <h3 id="access-control">Access Control</h3> <blockquote> <p>It used to prevent misuse of resources and ensure that only authorized users have access to the available rewources.</p> </blockquote> <p>=&gt; Prevent unauthorized use of a resource</p> <h4 id="examples">Examples</h4> <ul> <li><strong>Discretionary Access Control (DAC)</strong>: allows the owner or administrator of a resource to decide who is allowed to access it</li> <li><strong>Role-based Access Control (RBAC)</strong>: allows access based on the role of the user within the organization. ex) AWS account</li> <li><strong>Mandatory Access Control (MAC)</strong>: the access is granted based on a set of predefined rules</li> <li><strong>Attribute-based Access Control (ABAC)</strong>: the access is granted based on the attributes of the requestor, the resource, and the context in which the access request is made</li> </ul> <h3 id="data-confidentiality">Data Confidentiality</h3> <blockquote> <p>Responsible for ensuring that the data is <u>kept extremely safe from third-party intruders.</u></p> </blockquote> <p>=&gt; Protect data from unauthorized disclosure</p> <h4 id="types-of-data-confidentiality">Types of data confidentiality</h4> <ul> <li><strong>Protecting data in transit</strong>: to protect data that is being transmitted between two parties, over a network or through the internet. Can include both passive and active protection measures, encryption.</li> <li><strong>Protecting data at rest</strong>: to protect data that is <u>stored on a device or system</u>, on a HD or in a DB. Can include access controls, encryption, and backups to prevent unauthorized access or data loss.</li> <li> <p><strong>Protecting traffic flow from analysis</strong>: to protect the characteristics of the traffic flow, the source and destination, frequency, length and other characteristics of the traffic on a communication facility. Can include measures such as traffic padding and traffic encryption.</p> </li> <li>Data Confidentiality services are important for maintaining the privacy and security of sensitive information, and they can take many forms to protect data in transit, at rest and traffic flow from analysis.</li> </ul> <h3 id="data-integrity">Data Integrity</h3> <blockquote> <p>Ensure that the transmitted information received by the receiver is <strong>well-authenticated</strong> and there is <u>no tampering with the information received.</u></p> </blockquote> <p>=&gt; Assure data received are exactly as sent by authorized entity</p> <p>Helps to ensure that the data received is the same as the data that was sent and that it has not been <strong>modified</strong>, <strong>deleted</strong>, or <strong>tampered</strong> <u>with in any way.</u></p> <ul> <li>Data Integrity services are important for <u>maintaning the authenticity and accuracy of transmitted information</u>, and they can take many forms to protect a stream of messages, individual messages, or selected fields within a message.</li> </ul> <h3 id="nonrepudiation">Nonrepudiation</h3> <p>Used to prevent either sender or receiver of a transmitted message from denying that the message was sent or received.</p> <p>=&gt; Protect against denial of one entity involved in communications of having participated in communications</p> <ul> <li><strong>Message is sent</strong>: the receiver can prove that the alleged sender in fact sent the message</li> <li><strong>Message is received</strong>: the sender can prove that the alleged receiver in fact received the message</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Security/Security_Services-480.webp 480w,/assets/img/Security/Security_Services-800.webp 800w,/assets/img/Security/Security_Services-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Security/Security_Services.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="reference">Reference</h1> <ul> <li><em>CS458-Introduction to Information Security</em>. (2024). Sajad Meisami, Ph.D. Illinois Institute of Technology.</li> </ul>]]></content><author><name></name></author><category term="Info_Security"/><category term="WIL"/><summary type="html"><![CDATA[Concepts and understanding of Information Security]]></summary></entry><entry><title type="html">Security Basic - Easiest penetration &amp;amp; Defense of Computer Systems</title><link href="https://namdarine.github.io/blog/2024/Security-EasiestPenetration/" rel="alternate" type="text/html" title="Security Basic - Easiest penetration &amp;amp; Defense of Computer Systems"/><published>2024-09-19T18:00:00+00:00</published><updated>2024-09-19T18:00:00+00:00</updated><id>https://namdarine.github.io/blog/2024/Security-EasiestPenetration</id><content type="html" xml:base="https://namdarine.github.io/blog/2024/Security-EasiestPenetration/"><![CDATA[<h1 id="principle-of-easiest-penetration">Principle of Easiest Penetration</h1> <ul> <li>Security principle:<br/> &amp;nbsp - A system is only as secure as its weakest link<br/> &amp;nbsp - An attacker will often target the easiest poin of encry in a system rather than the most obvious one</li> </ul> <p>Emphasizes the importance of identifying and addressing potential vulnerabilities in a system. As well as testing and regularly reviewing the security controls that are in place.</p> <h2 id="devsecops-processing">DevSecOps processing</h2> <ol> <li>Initialization Requirement</li> <li>Design<br/> &amp;nbsp - Threat modeling: Secure Design, Archetiecture</li> <li>Build Implementation<br/> &amp;nbsp - Secure code practice</li> <li>Testing<br/> &amp;nbsp - Static Analysis, Dynamic Analysis, Interactive Analysis, SCA</li> <li>Deployment Release</li> <li>Maintenance</li> </ol> <p>Authentication \(\in\) Threat modeling</p> <h1 id="principle-of-adequate-protection">Principle of Adequate Protection</h1> <p>It highlights the need for balancing the cost of security measures with the potential impact of security incidents.<br/> Also, advises organizations not to overspend on security measures to protect a system that would only cause limited damage if compromise. This principle considers both technical and economic factors in implementing the most effective security measures to protect the assets.</p> <ul> <li>A proper security risk assessment</li> <li>To weigh the costs and the risks</li> <li>Make sure the right protection measures are in place, balance; Not too much or too little</li> </ul> <h1 id="balancing-information-security-and-access">Balancing information security and access</h1> <p>This requires a balance between ==protection and availability==, known as the security vs. accessibility trade-off.<br/> Includes balancing the need to protect sensitive information with the need to allow employees, customers, and other stakeholders access to that information.</p> <ul> <li>The goal is to find a balance that ensures the ==confidentiality, integrity, and availability of information== while enabling the organization to function effectively.</li> </ul> <h1 id="cryptography">Cryptography</h1> <blockquote> <p>Cyprtography is the practice of protecting data by convering it into an unreadable format (encryption) that can only be accessed by someone who has the proper decryption key.</p> </blockquote> <h2 id="common-uses">Common uses</h2> <ul> <li>Protecting data by making it unreadable through the use of encryption algorithms</li> <li>Authenticating users with digital signatures<br/> &amp;nbsp - use a combination of encryption and hashing to prove the identity of the sender</li> <li>Authenticating transactions with cryptographic protocols, SSL/TLS<br/> &amp;nbsp - provide secure communication between web browsers and servers</li> <li>Ensuring the integrity of stored data by using cryptographic techniques<br/> &amp;nbsp ex) Message Authentication Codes (MAC)</li> <li>Aid customers’ privacy by having their personal information automatically become unreadable after a certain length of time</li> </ul> <h1 id="software-controls">Software controls</h1> <blockquote> <p>Security measure that are used to protect computer systems and networks from unauthorized access and other types of threats.</p> </blockquote> <h2 id="examples">Examples</h2> <ul> <li>Passwords and other forms of access control<br/> &amp;nbsp ex) biometric authentication</li> <li>Operating systems that include built-in security features<br/> &amp;nbsp ex) user accounts and permissions, be used to separate users’ actions from each other on a system</li> <li>Virus scanners watch for some kinds of malware</li> <li>Develpment controls enforce quality measures on the original source code</li> <li>Personal firewalls</li> </ul> <h1 id="hardware-controls">Hardware controls</h1> <blockquote> <p>Security measure that use separate hardware devices to protect computer systems and networks</p> </blockquote> <h2 id="examples-1">Examples</h2> <ul> <li>Biometric readers<br/> &amp;nbsp ex) fingerprint readers</li> <li>Smart tokens<br/> &amp;nbsp ex) small and portable decices that generate one-time passcodes</li> <li>Firewalls</li> <li>Intrusion detection systems</li> </ul> <h1 id="physical-controls">Physical controls</h1> <blockquote> <p>Security measure that are designed to protect the hardware itself and prvent physical access to the console, storage media, and other critical components of a computer system or network.</p> </blockquote> <h2 id="examples-2">Examples</h2> <ul> <li>Locks</li> <li>Security guards</li> <li>Off-site backups: To protect against the possibility of fire, flood, or other types of natural disasters</li> <li>Location-based controls<br/> &amp;nbsp ex) Do not put data center on a fault line in California</li> </ul> <h1 id="reference">Reference</h1> <ul> <li><em>CS458-Introduction to Information Security</em>. (2024). Sajad Meisami, Ph.D. Illinois Institute of Technology.</li> </ul>]]></content><author><name></name></author><category term="Info_Security"/><category term="WIL"/><summary type="html"><![CDATA[Concepts and understanding of Information Security]]></summary></entry></feed>