---
layout: post
title: RAG and LoRA
date: 2025-03-01 18:00:00
description: Understand the concept of RAG and LoRA
tags: WIS
categories: ML
toc:
  sidebar: left
---

# RAG (Retrieval-Augmented Generation)

## What is RAG?
RAG is a technique that helps to generate and up-to-date information by adding an information retrieval (Retrieval) function to the Large Language Model (LLM). While existing LLMs rely on pre-trained data to generate answers, RAG finds relevant information from an external database and augments the model's response.  
This is cost-effective approach to improve LLM results to maintain relevance, accuracy, and utility in various situation.   
  
## Key Components of RAG

### Retriever
- Searches for relevant documents from a **vector** database, such as FAISS, Pinecone, Weaviate, Qdrant  
- Uses BM25, Dense Passage Retrieval (DPR), or ColBERT for efficient information retrieval  
  
### Generator
- Uses the retrieved documents as context to generate text  
- Typically based on transformer models like GPT, MPT or LLaMA  
  
## How RAG Works
1. User Query $ \rightangle $ Vector Representation  
    - The input query is transformed into a vector and compared against pre-stored documents in a vector database  
  
2. Document Retrieval  
    - The retriever fetches relevant documents based on similarity  
  
3. LLM Response Generation  
    - The LLM uses retrieved documents as additional context to generate a response  
  
<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">
{% include figure.liquid loading="eager" path="assets/img/rag.jpg" class="img-fluid rounded z-depth-1" %}
</div>
</div>
  
## Advantages
- Incorporates real-time data without retraining the model. This reflects the latest information even the model was not trained about.  
- Reduces hallucinations by providing factual references. This improves reliability  
- Allows customization for specific domains, such as legal, medical, finance  
  
It can adapt to changing requirements or departmental use by controlling and changing the information source of LLM. It can also allow developers to limit the search for sensitive information to different levels of authentication and allow LLM to generate appropriate responses. It can also troubleshoot and correct issues when LLM references sources of misinformation for specific questions. Organizations can more confidently implement Generative AI technologies for a wider range of applications.
   
# LoRA (Low-Rank Adaptation)

## What is LoRA?
> Low-Rank Adaptation is lightweight fine-tuning method that adapts LLMs **without modifying their core weights**. Instead of traing the entire models, LoRA adds small trainable matrices to selected layers.  
  
## Key Concept of LoRA
Instead of updating directly the weight matrix of the transformer model, LoRA introduces two smaller matrices that capture only the necessary modifications. That is, a method of learning only small additional matrices while maintaining the main weights of the original model and changing only the necessary parts.  
This method drastically reduces memory usage and computational costs while maintaining fine-tuning effectiveness.  
  
## How LoRA Works
1. The pre-trained model weights (W) remain frozen and introduces low-rank matrices (A and B) to learn task-specific modifications.  
2. When there is input data, tune the model with the updated value of the original weights (W) + new low-rank matrices (A and B).   
3. After training, only saving the added matrices (A, B) can maintain the lightweight model while reducing GPU memory consumption.  
  
## Advantages of LoRA
- **Efficient memory usage**: requires significantly less GPU VRAM compared with Full Fine-Tuning  
- 

  
# Reference
- image: What Is Rag? - Retrieval-Augmented Generation AI Explained - AWS, aws.amazon.com/what-is/retrieval-augmented-generation/. Accessed 12 Mar. 2025. 