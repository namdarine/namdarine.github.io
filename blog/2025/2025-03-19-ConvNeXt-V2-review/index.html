<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote> <p>This post summarizes “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders” (Sanghyun Woo et al., CVPR 2023) and shares an implementation based on the ideas presented in the paper. This post was written for learning purposes.</p> </blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> <p>ConvNeXt V2 is an improved model of ConvNeXt, a modern convolutional neural network (ConvNet), which significantly improves performance by co-designing self-supervised learning using <strong>Masked Autoencoder (MAE)</strong> and architectural design. Existing ConvNeXt was optimized for supervised learning, but when simply combined with MAE, its performance did not meet expectations.<br> To solve this:</p> <ol> <li> <p>Fully Convolutional Masked Autoencoder (FCMAE): Transformer-optimized MAE transform to fit ConvNet. It handles masked inputs efficiently by introducing sparse convolution and replacing the decoder with a simple ConvNeXt block to make it a fully convolutional structure.</p> </li> <li> <p>Global Response Normalization (GRN): Add a new normalization layer that enhances feature competition between channels. This solved the feature collapse problem and increased expressiveness.</p> </li> </ol> <p>As a result, ConvNeXt V2 significantly improves the performance of pure ConvNet on various tasks such as ImageNet classification (up to 88.9% accuracy), COCO object detection and ADE20K segmentation. The model is scalable from 3.7M parameter (Atto) to 650M parameter (Huge), and achieves state-of-art performance with public data.</p> <h2 id="core-content-analysis">Core Content Analysis</h2> <h3 id="problem-definitions-and-mathematical-modeling">Problem definitions and Mathematical modeling</h3> <p>Existing ConvNeXt is well-optimized for supervised learning, but when combined with self-supervised learning, especially MAE, the performance is poor. Since the MAE is designed to be suitable for transformers, it creates compatibility issues with ConvNet’s dense sliding window scheme. Furthermore, the expressive power is degraded by “feature collapse” during mask-based pre-training on ConvNet.</p> <h3 id="defined-variables-and-expressions">Defined variables and expressions</h3> <ul> <li> <strong>Masking</strong>: Generate a random mask <strong><em>M</em></strong> with 60% mask rate for input image <strong><em>I</em></strong>. <strong><em>M</em></strong> is defined in units of $ 32 \times 32 $ patches at the last stage and is tailored to the original resolution by upsampling.</li> <li> <strong>Sparsal convolution</strong>: Processing only the pixels seen in input $ X \in \mathbb{R}^{H \times W \times C} $. Output $ Y = SparseConv(X, W) $, where $ W $ is learningable filters.</li> <li> <table> <tbody> <tr> <td> <strong>GRN</strong>: Global aggregation $ G(X) =</td> <td> </td> <td>X</td> <td> </td> <td>_2 $ for input $ X \in \mathbb{R}^{H \times W \times C} $, Normalization $ N(X) = \frac{X}{G(X)} $, Correction $ Y = \gamma \cdot N(X) + \beta $ ($ \gamma $, $ \beta $ are learnable parameters).</td> </tr> </tbody> </table> </li> <li> <strong>Loss function</strong>: $ L = MSE(I_{masked}, \hat{I}_{masked}) $ for masked patches, where $ \hat{I} $ is a reconstructed image.</li> </ul> <h3 id="assumtion">Assumtion</h3> <p>Suppose that if ConvNet is to be as suitable for mask-based self-supervised learning as Transformer, the architecture, and the learning framework must be co-designed. Assume that the feature collapse comes from a lack of competition between channels.</p> <h3 id="coonection-with-existing-research">Coonection with existing research</h3> <p>MAE has been successful on ViT (HE et al., 2022), but its effectiveness is limited on ConvNet (Jing et al., 2022). ConvNeXt V1 (Liu et al., 2022) has shown scalability in supervised learning, but self-supervised learning has been insufficient.</p> <h2 id="analyzing-the-suggested-methods">Analyzing the suggested methods</h2> <h3 id="core-algorithms-and-model-structures">Core Algorithms and Model Structures</h3> <ul> <li> <strong>FCMAE</strong>: Fully convolutional MAE. The encoder applies sparse convolution to ConvNeXt, and the decoder consists of a single ConvNeXt block (512 dimensions). Masked inputs are viewed as sparse data and processed.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure2-480.webp 480w,/assets/img/paper1/figure2-800.webp 800w,/assets/img/paper1/figure2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>GRN</strong>: Normalized layer added behind MLP layer. Global response (L2 norm) increases inter-channel contrast and enhances feature diversity.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/algorithm1-480.webp 480w,/assets/img/paper1/algorithm1-800.webp 800w,/assets/img/paper1/algorithm1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/algorithm1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>ConvNeXt V2</strong>: Integrate GRN into the ConvNeXt block and remove LayerScale. Model size expansion from Atto (3.7M) to Huge (650M).</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure5-480.webp 480w,/assets/img/paper1/figure5-800.webp 800w,/assets/img/paper1/figure5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure5.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="improvements-over-existing-research">Improvements over existing research</h3> <ul> <li>MAE (based on ViT) is optimized for transformers with an asymmetric encoder-decoder. FCMAE transforms to full convolution to fit ConvNet, improving efficiency with sparse convolution.</li> <li>SimMIM(based on Swin) and others are transformer-dependent. GRN resolves ConvNet-specific feature collapse and improves performance without additional parameters.</li> </ul> <h3 id="originality-of-the-technique">Originality of the technique</h3> <p>Sparse convolution is inspired by 3D point clouds (Choy et al., 2019), and GRN is an idea from lateral inhibition in neuroscience. Both are newly applied to ConvNet.</p> <h2 id="experimental-interpretations---key-concept-analysis">Experimental Interpretations - Key Concept Analysis</h2> <h3 id="experimental-methods-and-datasets">Experimental methods and datasets</h3> <h4 id="used-datasets">Used datasets</h4> <ul> <li> <strong>ImageNet-1K</strong>: 1,000 classes, 1.3 million images. 800 epochs of pre-training, 100 epochs of fine-tuning</li> <li> <strong>ImageNet-22K</strong>: 22,000 classes, used for intermediate fine tuning. $ 384 \times 384 $ resolution</li> <li> <strong>COCO</strong>: Object detection and instance segmentation, evaluated with Mask R-CNN</li> <li> <strong>ADE20K</strong>: Semantic segmentation, evaluated with UpperNet</li> <li> <strong>Preprocessing</strong>: Minimum data augmentation (random crop only), patch-wise normalization</li> </ul> <h4 id="experimental-environment">Experimental environment</h4> <ul> <li> <strong>Hardware</strong>: 256 core TPU-v3 pod (based on JAX)</li> <li> <strong>Software</strong>: PyTorch/JAX, https://github.com/rwightman/pytorch-image-models</li> </ul> <h4 id="performance-comparison-indicators">Performance comparison indicators</h4> <ul> <li> <strong>ImageNet</strong>: Top-1 accuracy (%)</li> <li> <strong>COCO</strong>: $ mAP^{box} $ (detection), $ mAP^{mask} $ (segmentation)</li> <li> <strong>ADE20K</strong>: $ mloU $ (Average intersection/union)</li> <li> <strong>Comparative method</strong>: Compare to V1 (supervised), Compare V2 + FCMAE, and Transformer (Swin, ViT)</li> </ul> <h2 id="result">Result</h2> <h3 id="summary">Summary</h3> <ul> <li> <strong>*ImageNet-1K</strong>: V2 + FCMAE is superior to supervised V1 (83.8% and 84.3%) with Base (84.6%), Large (85.6%), and Huge (86.3%). After IN-22K fine-tuning, Huge is 88.9% SOTA</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table5-480.webp 480w,/assets/img/paper1/table5-800.webp 800w,/assets/img/paper1/table5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table5.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>COCO</strong>: V2 + FCMAE Huge is $ mAP^{box} $ 2.5 and $ mAP^{mask} $ 2.0 higher than Swin Huge</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table6-480.webp 480w,/assets/img/paper1/table6-800.webp 800w,/assets/img/paper1/table6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table6.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>ADE20K</strong>: V2 + FCMAE Huge is mIoU 54.0, a significant improvement over supervised V1 (49.9)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table7-480.webp 480w,/assets/img/paper1/table7-800.webp 800w,/assets/img/paper1/table7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table7.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="reasons-for-performance-improvement">Reasons for performance improvement</h3> <ul> <li> <strong>FCMAE</strong>: Sparse convolutions prevent information leakage and simple decoders maintain efficiency</li> <li> <strong>GRN</strong>: Improve expressiveness with feature collapse resolution</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure3-480.webp 480w,/assets/img/paper1/figure3-800.webp 800w,/assets/img/paper1/figure3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure4-480.webp 480w,/assets/img/paper1/figure4-800.webp 800w,/assets/img/paper1/figure4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure4.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>V2 + FCMAE outperforms supervised V2 by 0.8 - 1.3%</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table3-480.webp 480w,/assets/img/paper1/table3-800.webp 800w,/assets/img/paper1/table3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>co-design</strong>: Synergy between architecture and learning framework achieves transformer-level performance.</li> </ul> <h2 id="generality">Generality</h2> <ul> <li>Not conditionally dependent: Consistent performance improvement at all sizes, from Atto to Huge</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure1-480.webp 480w,/assets/img/paper1/figure1-800.webp 800w,/assets/img/paper1/figure1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Verify robust transfer learning performance on various tasks (ImageNet, COCO, ADE20K)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table6-480.webp 480w,/assets/img/paper1/table6-800.webp 800w,/assets/img/paper1/table6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table6.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table7-480.webp 480w,/assets/img/paper1/table7-800.webp 800w,/assets/img/paper1/table7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table7.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>However, the Huge model may be slightly data-scale dependent, as it has benefited more significantly from the IN-22K additional data</li> </ul> </body></html>