<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ConvNeXt V2 Paper Review | namdarine </title> <meta name="author" content="Nam Gyu Lee"> <meta name="description" content="Paper review"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://namdarine.github.io/blog/2025/ConvNeXt-V2-review/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> namdarine </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">ConvNeXt V2 Paper Review</h1> <p class="post-meta"> March 19, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/wis"> <i class="fa-solid fa-hashtag fa-sm"></i> WIS</a>     ·   <a href="/blog/category/cv"> <i class="fa-solid fa-tag fa-sm"></i> CV,</a>   <a href="/blog/category/paper-review"> <i class="fa-solid fa-tag fa-sm"></i> paper_review</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>This post summarizes “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders” (Sanghyun Woo et al., CVPR 2023) and shares an implementation based on the ideas presented in the paper. This post was written for learning purposes.</p> </blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> <p>ConvNeXt V2 is an improved model of ConvNeXt, a modern convolutional neural network (ConvNet), which significantly improves performance by co-designing self-supervised learning using <strong>Masked Autoencoder (MAE)</strong> and architectural design. Existing ConvNeXt was optimized for supervised learning, but when simply combined with MAE, its performance did not meet expectations.<br> To solve this:</p> <ol> <li> <p>Fully Convolutional Masked Autoencoder (FCMAE): Transformer-optimized MAE transform to fit ConvNet. It handles masked inputs efficiently by introducing sparse convolution and replacing the decoder with a simple ConvNeXt block to make it a fully convolutional structure.</p> </li> <li> <p>Global Response Normalization (GRN): Add a new normalization layer that enhances feature competition between channels. This solved the feature collapse problem and increased expressiveness.</p> </li> </ol> <p>As a result, ConvNeXt V2 significantly improves the performance of pure ConvNet on various tasks such as ImageNet classification (up to 88.9% accuracy), COCO object detection and ADE20K segmentation. The model is scalable from 3.7M parameter (Atto) to 650M parameter (Huge), and achieves state-of-art performance with public data.</p> <h2 id="core-content-analysis">Core Content Analysis</h2> <h3 id="problem-definitions-and-mathematical-modeling">Problem definitions and Mathematical modeling</h3> <p>Existing ConvNeXt is well-optimized for supervised learning, but when combined with self-supervised learning, especially MAE, the performance is poor. Since the MAE is designed to be suitable for transformers, it creates compatibility issues with ConvNet’s dense sliding window scheme. Furthermore, the expressive power is degraded by “feature collapse” during mask-based pre-training on ConvNet.</p> <h3 id="defined-variables-and-expressions">Defined variables and expressions</h3> <ul> <li> <p><strong>Masking</strong>: Generate a random mask <em>M</em> with 60% mask rate for input image <em>I</em>. <em>M</em> is defined in units of \(32 \times 32\) patches at the last stage and is tailored to the original resolution by upsampling.</p> </li> <li> <p><strong>Sparsal convolution</strong>: Processing only the pixels seen in input \(X \in \mathbb{R}^{H \times W \times C}\). Output \(Y = SparseConv(X, W)\), where <em>W</em> is learningable filters.</p> </li> <li> <p><strong>GRN</strong>: Global aggregation \(G(X) = \vert\vert X \vert\vert\_{2}\) for input \(X \in \mathbb{R}^{H \times W \times C}\), Normalization \(N(X) = \frac{X}{G(X)}\), Correction \(Y = \gamma \cdot N(X) + \beta\) (\(\gamma\), \(\beta\) are learnable parameters).</p> </li> <li> <p><strong>Loss function</strong>: \(L = MSE({I}^{masked}, {\hat{I}}^{masked})\) for masked patches, where \(\hat{I}\) is a reconstructed image.</p> </li> </ul> <h3 id="assumtion">Assumtion</h3> <p>Suppose that if ConvNet is to be as suitable for mask-based self-supervised learning as Transformer, the architecture, and the learning framework must be co-designed. Assume that the feature collapse comes from a lack of competition between channels.</p> <h3 id="coonection-with-existing-research">Coonection with existing research</h3> <p>MAE has been successful on ViT (HE et al., 2022), but its effectiveness is limited on ConvNet (Jing et al., 2022). ConvNeXt V1 (Liu et al., 2022) has shown scalability in supervised learning, but self-supervised learning has been insufficient.</p> <h2 id="analyzing-the-suggested-methods">Analyzing the suggested methods</h2> <h3 id="core-algorithms-and-model-structures">Core Algorithms and Model Structures</h3> <ul> <li> <strong>FCMAE</strong>: Fully convolutional MAE. The encoder applies sparse convolution to ConvNeXt, and the decoder consists of a single ConvNeXt block (512 dimensions). Masked inputs are viewed as sparse data and processed.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure2-480.webp 480w,/assets/img/paper1/figure2-800.webp 800w,/assets/img/paper1/figure2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>GRN</strong>: Normalized layer added behind MLP layer. Global response (L2 norm) increases inter-channel contrast and enhances feature diversity.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/algorithm1-480.webp 480w,/assets/img/paper1/algorithm1-800.webp 800w,/assets/img/paper1/algorithm1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/algorithm1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>ConvNeXt V2</strong>: Integrate GRN into the ConvNeXt block and remove LayerScale. Model size expansion from Atto (3.7M) to Huge (650M).</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure5-480.webp 480w,/assets/img/paper1/figure5-800.webp 800w,/assets/img/paper1/figure5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure5.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="improvements-over-existing-research">Improvements over existing research</h3> <ul> <li>MAE (based on ViT) is optimized for transformers with an asymmetric encoder-decoder. FCMAE transforms to full convolution to fit ConvNet, improving efficiency with sparse convolution.</li> <li>SimMIM(based on Swin) and others are transformer-dependent. GRN resolves ConvNet-specific feature collapse and improves performance without additional parameters.</li> </ul> <h3 id="originality-of-the-technique">Originality of the technique</h3> <p>Sparse convolution is inspired by 3D point clouds (Choy et al., 2019), and GRN is an idea from lateral inhibition in neuroscience. Both are newly applied to ConvNet.</p> <h2 id="experimental-interpretations---key-concept-analysis">Experimental Interpretations - Key Concept Analysis</h2> <h3 id="experimental-methods-and-datasets">Experimental methods and datasets</h3> <h4 id="used-datasets">Used datasets</h4> <ul> <li> <strong>ImageNet-1K</strong>: 1,000 classes, 1.3 million images. 800 epochs of pre-training, 100 epochs of fine-tuning</li> <li> <strong>ImageNet-22K</strong>: 22,000 classes, used for intermediate fine tuning. \(384 \times 384\) resolution</li> <li> <strong>COCO</strong>: Object detection and instance segmentation, evaluated with Mask R-CNN</li> <li> <strong>ADE20K</strong>: Semantic segmentation, evaluated with UpperNet</li> <li> <strong>Preprocessing</strong>: Minimum data augmentation (random crop only), patch-wise normalization</li> </ul> <h4 id="experimental-environment">Experimental environment</h4> <ul> <li> <strong>Hardware</strong>: 256 core TPU-v3 pod (based on JAX)</li> <li> <strong>Software</strong>: PyTorch/JAX, <a href="https://github.com/rwightman/pytorch-image-models" rel="external nofollow noopener" target="_blank">github</a> </li> </ul> <h4 id="performance-comparison-indicators">Performance comparison indicators</h4> <ul> <li> <strong>ImageNet</strong>: Top-1 accuracy (%)</li> <li> <strong>COCO</strong>: \(mAP^{box}\) (detection), \(mAP^{mask}\) (segmentation)</li> <li> <strong>ADE20K</strong>: \(mloU\) (Average intersection/union)</li> <li> <strong>Comparative method</strong>: Compare to V1 (supervised), Compare V2 + FCMAE, and Transformer (Swin, ViT)</li> </ul> <h2 id="result">Result</h2> <h3 id="summary">Summary</h3> <ul> <li> <strong>ImageNet-1K</strong>: V2 + FCMAE is superior to supervised V1 (83.8% and 84.3%) with Base (84.6%), Large (85.6%), and Huge (86.3%). After IN-22K fine-tuning, Huge is 88.9% SOTA</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table5-480.webp 480w,/assets/img/paper1/table5-800.webp 800w,/assets/img/paper1/table5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table5.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>COCO</strong>: V2 + FCMAE Huge is \(mAP^{box}\) 2.5 and \(mAP^{mask}\) 2.0 higher than Swin Huge</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table6-480.webp 480w,/assets/img/paper1/table6-800.webp 800w,/assets/img/paper1/table6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table6.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>ADE20K</strong>: V2 + FCMAE Huge is mIoU 54.0, a significant improvement over supervised V1 (49.9)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table7-480.webp 480w,/assets/img/paper1/table7-800.webp 800w,/assets/img/paper1/table7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table7.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="reasons-for-performance-improvement">Reasons for performance improvement</h3> <ul> <li> <strong>FCMAE</strong>: Sparse convolutions prevent information leakage and simple decoders maintain efficiency</li> <li> <strong>GRN</strong>: Improve expressiveness with feature collapse resolution</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure3-480.webp 480w,/assets/img/paper1/figure3-800.webp 800w,/assets/img/paper1/figure3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure4-480.webp 480w,/assets/img/paper1/figure4-800.webp 800w,/assets/img/paper1/figure4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure4.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>V2 + FCMAE outperforms supervised V2 by 0.8 - 1.3%</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table3-480.webp 480w,/assets/img/paper1/table3-800.webp 800w,/assets/img/paper1/table3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>co-design</strong>: Synergy between architecture and learning framework achieves transformer-level performance.</li> </ul> <h2 id="generality">Generality</h2> <ul> <li>Not conditionally dependent: Consistent performance improvement at all sizes, from Atto to Huge</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/figure1-480.webp 480w,/assets/img/paper1/figure1-800.webp 800w,/assets/img/paper1/figure1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/figure1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Verify robust transfer learning performance on various tasks (ImageNet, COCO, ADE20K)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table6-480.webp 480w,/assets/img/paper1/table6-800.webp 800w,/assets/img/paper1/table6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table6.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper1/table7-480.webp 480w,/assets/img/paper1/table7-800.webp 800w,/assets/img/paper1/table7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/paper1/table7.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>However, the Huge model may be slightly data-scale dependent, as it has benefited more significantly from the IN-22K additional data</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ML-Final_Prep/">Machine Learning Final Preparation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Binary-Search-Tree/">Binary Search Tree (BST)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ANN/">Artificial Neural Network</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Perceptron/">Artificial Neural Network - Perceptron</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Amortized-Analysis/">Amortized Analysis</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Nam Gyu Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>